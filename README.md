# InformationRetrievalBasic
## Gaffarov Marat, group 11-208
## Minullin Timur, group 11-209

---
# Deployment Manual

## Requirements

- [.NET 8 SDK](https://dotnet.microsoft.com/download/dotnet/8.0)

## How to run project

Находясь в директории проекта выполнить кманду:

``` bash
dotnet run
```

---
# Release Notes 

## Common Lib

### TextProcessor

`TextProcessor` — это статический класс, предназначенный для извлечения и предварительной обработки текста из HTML-документов. Он используется в задачах, связанных с краулингом, анализом текста, токенизацией и подготовкой данных для последующей обработки (например, лемматизации или индексации).

#### Функциональность

##### 1. **Извлечение текста из HTML**
Метод `ExtractTextFromHtmlFile` принимает путь к файлу, содержащему HTML-разметку, и возвращает только **видимый текст**, очищенный от тегов, комментариев и прочей разметки. Используется библиотека `HtmlAgilityPack` для надежного парсинга HTML.

##### 2. **Токенизация**
Метод `Tokenize` принимает текст и:
- Извлекает слова, состоящие **только из латинских букв**.
- Приводит их к **нижнему регистру**.
- Исключает:
  - Стоп-слова (предлоги, союзы, местоимения и т.д.).
  - Токены короче заданного порога (по умолчанию — 2 символа).
- Возвращает перечисление строк (`IEnumerable<string>`), готовое для дальнейшей обработки.

##### 3. **Стоп-слова**
Класс содержит встроенный набор стоп-слов на **английском языке**, который можно использовать для фильтрации часто встречающихся, но семантически незначимых слов.


### BooleanQueryParser

`BooleanQueryParser` — это класс, реализующий **парсер и вычислитель булевых запросов** с поддержкой логических операторов `AND`, `OR`, `NOT`, а также **скобок** и приоритетов операций. Используется для обработки поисковых запросов в системах, основанных на инвертированном индексе.

#### Функциональность

##### 1. **Поддержка булевых операторов**
- `AND` — логическое "И". Возвращает документы, содержащие оба термина.
- `OR` — логическое "ИЛИ". Возвращает документы, содержащие хотя бы один из терминов.
- `NOT` — логическое "НЕ". Исключает документы, содержащие указанный термин.

##### 2. **Поддержка скобок**
- Позволяет группировать части запроса для изменения порядка вычисления.
- Пример: `(A OR B) AND NOT C`.

##### 3. **Приоритеты операций**
- `NOT` — наивысший приоритет.
- `AND` — средний приоритет.
- `OR` — наименьший приоритет.

##### 4. **Вычисление на основе инвертированного индекса**
- Принимает словарь, где ключ — термин, значение — множество ID документов, содержащих этот термин.
- Возвращает множество ID документов, удовлетворяющих запросу.


### VectorSearchEngine

`VectorSearchEngine` — это класс, реализующий **векторный поиск** по коллекции документов с использованием **TF-IDF векторов**. Используется для поиска релевантных документов по текстовому запросу на основе косинусного сходства.

#### Функциональность

##### 1. **Загрузка TF-IDF векторов**
- Автоматически загружает TF-IDF вектора документов из директории, сгенерированной в `HW4`.
- Ожидает файлы вида `*_terms.txt`, содержащие строки формата: `термин idf tfidf`.
- Восстанавливает имена документов из имён файлов (например, `10_terms.txt` → `10.txt`).

##### 2. **Векторный поиск**
- Преобразует текстовый запрос в TF-вектор (частоты терминов в запросе).
- Вычисляет **косинусное сходство** между вектором запроса и векторами документов.
- Возвращает топ-N результатов, отсортированных по убыванию релевантности.

##### 3. **Результаты поиска**
- Класс `SearchResult` содержит:
  - `DocumentName` — имя документа (например, `"10.txt"`).
  - `Score` — косинусное сходство (чем больше, тем релевантнее документ).

---

## Web Crawler Homework (1st Task)

Этот проект представляет собой простой веб-краулер на C#, написанный в рамках учебного задания. Он скачивает HTML-страницы из предварительно заданного списка, сохраняет их в текстовые файлы и создает индексный файл.

- Скачивает 100 HTML-страниц из списка `urls.json`.
- Сохраняет содержимое каждой страницы (вместе с HTML-разметкой) в отдельный файл в папке `crawled_pages`.
- Создает файл `index.txt`, где каждая строка содержит номер файла и соответствующий URL.

После выполнения программы в директории проекта появятся:
Папка `crawled_pages/`, содержащая файлы `1.txt`, `2.txt`, ..., каждый из которых — содержимое соответствующей скачанной страницы.
Файл `index.txt`, в котором каждая строка имеет вид: `<номер_файла> <URL>`.

Также был написан метод, который очищает скаченные страницы, от тегов `script`, `style`, `link`, `noscript`, `iframe`, `meta`. При этом сохраняет html структуру страницы для последующих задач.


## Page Processing Homework (2nd Task)

Вторая домашняя работа реализует **предварительную обработку и токенизацию** текста из HTML-документов, скачанных в первой части. Также выполняется **лемматизация** слов с группировкой по леммам.

### Функциональность

#### 1. **Чтение HTML-документов**
- Считывает файлы, полученные на первом этапе (например, `page_*.txt`).
- Очищает содержимое от HTML-тегов, комментариев и разметки с помощью `HtmlAgilityPack`.

#### 2. **Токенизация текста**
- Извлекает слова, состоящие **только из латинских букв**.
- Приводит к нижнему регистру.
- Удаляет:
  - Стоп-слова (предлоги, союзы, местоимения и т.д.).
  - Слова короче 2 символов.
  - Числа и слова с цифрами.

#### 3. **Лемматизация**
- Применяет стемминг (по-английски: stemming) с использованием `Snowball.Net`.
- Группирует токены по их леммам (например, "running", "ran" → "run").

#### 4. **Создание выходных файлов**
- `tokens.txt`: список уникальных токенов, по одному на строку.
- `lemmatized_tokens.txt`: лемма и связанные с ней токены в одной строке через пробел.

### Использование

1. Убедитесь, что в директории проекта находятся HTML-файлы (например, `page_001.txt`, `page_002.txt` и т.д.).
2. Запустите программу.
3. После завершения будут созданы два файла:
   - `tokens.txt`
   - `lemmatized_tokens.txt`

### Зависимости

- `HtmlAgilityPack` — для очистки HTML.
- `Snowball.Net` — для лемматизации (STEMMING).
- Использует класс `TextProcessor` из `Common` для обработки текста.

### Результаты работы

- Файл `tokens.txt` — список уникальных токенов.
- Файл `lemmatized_tokens.txt` — леммы и связанные с ними токены.

--- 

## Boolean Query Homework (3rd Task)

Третья домашняя работа реализует **построение инвертированного индекса** и **булев поиск** по коллекции документов, полученных на втором этапе (HW2).

### Функциональность

#### 1. **Построение инвертированного индекса**
- Считывает файлы с токенами, сгенерированные в `HW2` (из папки `tokens_per_doc`).
- Для каждого уникального токена формирует список ID документов, в которых он встречается.
- Хранит соответствие ID документа и его оригинального имени файла.

#### 2. **Сохранение индекса в файл**
- Сохраняет инвертированный индекс в файл `inverted_index.txt`.
- Формат: `термин: id1, id2, id3`.

#### 3. **Интерактивный булев поиск**
- Поддерживает логические операторы: `AND`, `OR`, `NOT`.
- Поддерживает **скобки** и **приоритеты** операций.
- Пример запроса: `(html AND markup) OR css`.
- Выводит список файлов, удовлетворяющих запросу.

#### 4. **Использование `BooleanQueryParser`**
- Использует класс `BooleanQueryParser` для разбора и вычисления булевых выражений.
- Операторы чувствительны к регистру (преобразуются в верхний перед обработкой).
- Генерирует дерево разбора и вычисляет результат с использованием инвертированного индекса.

### Использование

1. Убедитесь, что в папке `HW2/tokens_per_doc` находятся файлы вида `*_tokens.txt`.
2. Запустите программу.
3. Введите булев запрос, например:
``` bash
Query > (html AND markup) OR css
```
4. Получите список файлов, удовлетворяющих условию.

### Зависимости

- Использует класс `TextProceinverted_indexssor` из `Common` для токенизации и очистки текста.
- Использует класс `BooleanQueryParser` из `Common` для разбора запросов.

### Результаты работы

- Файл `inverted_index.txt` — инвертированный индекс.
- Интерактивный режим поиска с поддержкой сложных запросов.


## TF-IDF Processing Homework (4th Task)

Четвёртая домашняя работа реализует **расчёт TF-IDF (Term Frequency-Inverse Document Frequency)** для коллекции документов, используя результаты предыдущих этапов.

### Функциональность

#### 1. **Использование результатов HW2**
- Читает токены и леммы, сгенерированные в `HW2`:
  - `tokens_per_doc` — токены по документам.
  - `lemmas_per_doc` — леммы по документам.
  - `lemmas.txt` — маппинг термин → лемма.

#### 2. **Расчёт Document Frequency (DF)**
- Для каждого термина и леммы подсчитывает, **в скольких документах** они встречаются.
- Используется для расчёта IDF (Inverse Document Frequency).

#### 3. **Расчёт TF-IDF**
- Для каждого документа:
  - Считает **TF (Term Frequency)** — частоту термина/леммы в документе.
  - Считает **IDF (Inverse Document Frequency)** — редкость термина/леммы во всей коллекции.
  - Вычисляет **TF-IDF** = `TF * IDF`.

#### 4. **Создание выходных файлов**
- Для каждого документа создаются два файла:
  - `{doc_id}_terms.txt` — TF-IDF для терминов.
  - `{doc_id}_lemmas.txt` — TF-IDF для лемм.
- Формат строк:
  - `термин idf tfidf`
  - `лемма idf tfidf`

### Использование

1. Убедитесь, что:
   - В `HW1/crawled_pages` находятся исходные HTML-файлы.
   - В `HW2/tokens_per_doc` и `HW2/lemmas_per_doc` — результаты `HW2`.
   - В `HW2/lemmas.txt` — маппинг термин → лемма.
2. Запустите программу.
3. Результаты будут сохранены в папку `lemmas_terms_per_doc`.

### Формулы

- **TF** = `(количество вхождений термина в документе) / (общее количество терминов в документе)`
- **IDF** = `log(N / (1 + df))`, где:
  - `N` — общее количество документов.
  - `df` — количество документов, содержащих термин.
- **TF-IDF** = `TF * IDF`

### Зависимости

- Использует `TextProcessor` из `Common` для токенизации.
- Опирается на результаты `HW2` и `HW1`.

### Результаты работы

- Файлы `{doc_id}_terms.txt` и `{doc_id}_lemmas.txt` с рассчитанными значениями TF-IDF.


## Vector searching Homework (5th Task)

Пятая домашняя работа реализует **векторный поиск** по коллекции документов с использованием **TF-IDF векторов**, рассчитанных на предыдущем этапе (HW4).

### Функциональность

#### 1. **Загрузка TF-IDF векторов**
- Читает файлы, сгенерированные в `HW4` (из папки `lemmas_terms_per_doc`).
- Для каждого документа формирует вектор TF-IDF на основе:
  - Терминов и/или лемм.
  - Их весов (значений TF-IDF).

#### 2. **Векторный поиск**
- Преобразует текстовый запрос в TF-IDF вектор (аналогично документам).
- Вычисляет **косинусное сходство** между вектором запроса и векторами документов.
- Возвращает список документов, **отсортированных по релевантности**.

#### 3. **Интерактивный режим поиска**
- Пользователь вводит текстовый запрос.
- Программа выводит топ-N наиболее релевантных документов с их оценками.

### Использование

1. Убедитесь, что в `HW4/lemmas_terms_per_doc` находятся файлы вида:
   - `1_terms.txt`, `1_lemmas.txt`, `2_terms.txt`, и т.д.
2. Запустите программу.
3. Введите запрос.
4. Получите список документов, отсортированных по релевантности.

### Алгоритм

- **Косинусное сходство** используется для измерения схожести между векторами.
- Вектора нормализуются для корректного сравнения.
- Результаты сортируются по убыванию оценки (score).

### Зависимости

- Использует `TextProcessor` из `Common` для токенизации запроса.
- Опирается на результаты `HW4`.

### Результаты работы

- Интерактивный режим поиска.
- Возможность ранжирования документов по релевантности запросу.


